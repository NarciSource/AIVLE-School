{"cells":[{"cell_type":"markdown","metadata":{"id":"3xIPZjFU5rjt"},"source":["\n","\n","\n","\n","#**Aivle 스쿨 지원 질문, 답변 챗봇 만들기**\n","# 단계2 : 모델링"]},{"cell_type":"markdown","metadata":{"id":"-FPypzell2uc"},"source":["## 0.미션"]},{"cell_type":"markdown","metadata":{"id":"AC6wpFtQtR5y"},"source":["* 다음 두가지 챗봇을 만들고 비교해 봅시다.\n","* 챗봇1. Word2Vec 임베딩 벡터 기반 머신러닝 분류 모델링\n","    * Word2Vec 모델을 만들고 임베딩 벡터를 생성합니다.\n","    * 임베딩 벡터를 이용하여 intent를 분류하는 모델링을 수행합니다.\n","        * 이때, LightGBM을 추천하지만, 다른 알고리즘을 이용할수 있습니다.\n","    * 예측된 intent의 답변 중 임의의 하나를 선정하여 출력합니다.\n","* 챗봇2. 단계별 모델링1\n","    * 1단계 : type(일상대화 0, 에이블스쿨Q&A 1) 분류 모델 만들기\n","        * Embedding + LSTM 모델링\n","    * 2단계 : FastText 모델 생성하여 train의 임베딩벡터 저장\n","    * 3단계 : 코사인 유사도로 intent 찾아 답변 출력\n","        * 새로운 문장의 임베딩벡터와 train의 임베딩 벡터간의 코사인 유사도 계산\n","        * 가장 유사도가 높은 질문의 intent를 찾아 답변 출력하기\n","\n","* 챗봇 2개에 대해서 몇가지 질문을 입력하고 각각의 답변을 비교해 봅시다.\n"]},{"cell_type":"markdown","metadata":{"id":"pvBAaxSgrkt7"},"source":["## 1.환경준비"]},{"cell_type":"markdown","metadata":{"id":"AvBsTv3s-a3X"},"source":["### (1)라이브러리 설치"]},{"cell_type":"markdown","metadata":{"id":"EdhloaEO-mFZ"},"source":["#### 1) gensim 설치"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15613,"status":"ok","timestamp":1696550175072,"user":{"displayName":"Seonil Kwon","userId":"10539738350691841262"},"user_tz":-540},"id":"Pkk8tlp2-Q3g","outputId":"f4b76ec7-8d74-4ad8-857b-5ed01b84dd5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"]}],"source":["#gensim은 자연어 처리를 위한 오픈소스 라이브러리입니다. 토픽 모델링, 워드 임베딩 등 다양한 자연어 처리 기능을 제공\n","!pip install gensim"]},{"cell_type":"markdown","metadata":{"id":"w9I84wAV-EQ7"},"source":["#### 2) 형태소 분석을 위한 라이브러리"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52844,"status":"ok","timestamp":1696550227908,"user":{"displayName":"Seonil Kwon","userId":"10539738350691841262"},"user_tz":-540},"id":"eRgLSScn0QFG","outputId":"aa0ffc6a-d6d5-42bd-a1c1-3ec6c245cea7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","curl is already the newest version (7.81.0-1ubuntu1.13).\n","git is already the newest version (1:2.34.1-1ubuntu1.10).\n","0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","build-essential is already the newest version (12.9ubuntu3).\n","0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","cmake is already the newest version (3.22.1-1ubuntu1.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","g++ is already the newest version (4:11.2.0-1ubuntu1).\n","0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","flex is already the newest version (2.6.4-8build2).\n","0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","bison is already the newest version (2:3.8.2+dfsg-1build1).\n","0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","Package python-dev is not available, but is referred to by another package.\n","This may mean that the package is missing, has been obsoleted, or\n","is only available from another source\n","However the following packages replace it:\n","  python2-dev python2 python-dev-is-python3\n","\n","E: Package 'python-dev' has no installation candidate\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (3.0.2)\n","Requirement already satisfied: mecab-python in /usr/local/lib/python3.10/dist-packages (1.0.0)\n","Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.10/dist-packages (from mecab-python) (1.0.8)\n"]}],"source":["# mecab 설치를 위한 관련 패키지 설치\n","!apt-get install curl git\n","!apt-get install build-essential\n","!apt-get install cmake\n","!apt-get install g++\n","!apt-get install flex\n","!apt-get install bison\n","!apt-get install python-dev\n","!pip install cython\n","!pip install mecab-python"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7zwNEdPo3Rpm","executionInfo":{"status":"ok","timestamp":1696550234824,"user_tz":-540,"elapsed":6921,"user":{"displayName":"Seonil Kwon","userId":"10539738350691841262"}},"outputId":"3cb80483-49fa-461b-c463-aaa1ec285429"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n","mecab-ko is already installed\n","mecab-ko-dic is already installed\n","mecab-python is already installed\n","Done.\n"]}],"source":["# 형태소 기반 토크나이징 (Konlpy)\n","!python3 -m pip install konlpy\n","# mecab (ubuntu: linux, mac os 기준)\n","# 다른 os 설치 방법 및 자세한 내용은 다음 참고: https://konlpy.org/ko/latest/install/#id1\n","!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n","# !pip install mecab"]},{"cell_type":"markdown","metadata":{"id":"FlRWJB2w6Ip6"},"source":["### (2) 라이브러리 불러오기"]},{"cell_type":"markdown","metadata":{"id":"-TRDCUpP6Ip6"},"source":["* 세부 요구사항\n","    - 기본적으로 필요한 라이브러리를 import 하도록 코드가 작성되어 있습니다.\n","    - 필요하다고 판단되는 라이브러리를 추가하세요."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h1IYbPd_6Ip6"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import joblib\n","\n","# 필요하다고 판단되는 라이브러리를 추가하세요.\n","import os\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","from lightgbm import LGBMClassifier\n","from sklearn.metrics import *\n","\n","import tensorflow as tf\n","from keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout\n","from keras import Input, Model\n","from keras import optimizers\n","from keras.models import Sequential, load_model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"]},{"cell_type":"markdown","metadata":{"id":"ERab2qbnVloB"},"source":["* 형태소 분석을 위한 함수를 제공합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGr3phdYVloC"},"outputs":[],"source":["from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma\n","\n","# 다양한 토크나이저를 사용할 수 있는 함수\n","def get_tokenizer(tokenizer_name):\n","    if tokenizer_name == \"komoran\":\n","        tokenizer = Komoran()\n","    elif tokenizer_name == \"okt\":\n","        tokenizer = Okt()\n","    elif tokenizer_name == \"mecab\":\n","        tokenizer = Mecab()\n","    elif tokenizer_name == \"hannanum\":\n","        tokenizer = Hannanum()\n","    else:\n","        # \"kkma\":\n","        tokenizer = Kkma()\n","\n","    return tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1kGJuX6VloD"},"outputs":[],"source":["# 형태소 분석을 수행하는 함수\n","\n","def tokenize(tokenizer_name, original_sent, nouns=False):\n","    # 미리 정의된 몇 가지 tokenizer 중 하나를 선택\n","    tokenizer = get_tokenizer(tokenizer_name)\n","\n","    # tokenizer를 이용하여 original_sent를 토큰화하여 tokenized_sent에 저장하고, 이를 반환합니다.\n","    sentence = original_sent.replace('\\n', '').strip()\n","    if nouns:\n","        # tokenizer.nouns(sentence) -> 명사만 추출\n","        tokens = tokenizer.nouns(sentence)\n","    else:\n","        tokens = tokenizer.morphs(sentence)\n","    tokenized_sent = ' '.join(tokens)\n","\n","    return tokenized_sent"]},{"cell_type":"markdown","metadata":{"id":"wsLDv9tZc_i1"},"source":["### (3) 데이터 로딩\n","* 전처리 단계에서 생성한 데이터들을 로딩합니다.\n","    * train, test\n","    * 형태소분석 결과 데이터 : clean_train_questions, clean_test_questions\n","* Google Colab 환경에서 진행을 권장합니다.\n","    * 구글 드라이브 바로 밑에 project 폴더를 만들고,\n","    * 데이터 파일을 복사해 넣습니다."]},{"cell_type":"markdown","metadata":{"id":"T4kj7c2prtzc"},"source":["#### 1) Google Colab 환경 구축"]},{"cell_type":"markdown","metadata":{"id":"Uc_kIeeJeDgi"},"source":["* 구글 드라이브 연결"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2167,"status":"ok","timestamp":1696550250675,"user":{"displayName":"Seonil Kwon","userId":"10539738350691841262"},"user_tz":-540},"id":"dd0SPbYdfhS9","outputId":"224eca1c-3e68-4db1-afdb-c8d423e459fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5OIDazoeIN4"},"outputs":[],"source":["#ex : path = '/content/drive/MyDrive/project/'\n","path ="]},{"cell_type":"markdown","metadata":{"id":"GH3ApIzofYPb"},"source":["#### 2) 저장된 데이터 읽어오기\n","* 저장된 .pkl 파일들을 불러옵니다.\n","* 불러 온 후에는 shape를 확인해 봅시다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FT_JFnclfcQ4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjQCIzsUsKJD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sY4abSWpW4lb"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"mMQvj3AmIDEy"},"source":["## 2.챗봇1"]},{"cell_type":"markdown","metadata":{"id":"_x9L3XAfJPAh"},"source":["* **상세요구사항**\n","    * Word2Vec을 활용한 LightGBM 모델링(intent 분류)\n","        * Word2Vec을 이용하여 임베딩벡터 생성하기\n","            * Word Embedding으로 문장벡터 구하기\n","        * 임베딩 벡터를 이용하여 ML기반 모델링 수행하기\n","            * LightGBM 권장(다른 알고리즘을 이용할수 있습니다.)\n","    * 챗봇 : 모델의 예측결과(intent)에 따라 답변하는 챗봇 만들기\n","        * 질문을 입력받아, 답변하는 함수 생성"]},{"cell_type":"markdown","metadata":{"id":"1lQMnaY2SIKM"},"source":["### (1) Word2Vec을 이용하여 임베딩벡터 생성하기\n","* 'mecab' 형태소 분석기를 이용하여 문장을 tokenize\n","    * Word2Vec 모델을 만들기 위해서 입력 데이터는 리스트 형태여야 합니다.\n","    * 그래서 다시 리스트로 저장되도록 토크나이즈 해 봅시다.\n","* Word Embedding으로 문장벡터를 생성합니다.\n","    * 먼저 Word2Vec 모델을 만들고, train의 질문들을 문장벡터로 만듭시다.\n"]},{"cell_type":"markdown","metadata":{"id":"XZtMR3HVRfCI"},"source":["#### 1) 'mecab' 형태소 분석기를 이용하여 문장을 tokenize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FHwKdpLvJPc-"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ZsDPBh8Nhxy2"},"source":["#### 2) Word Embedding으로 문장벡터 구하기\n","* Word2Vec\n","    * 위에서 저장한 입력 데이터를 사용하여 Word2Vec 모델이 생성\n","    * 모델은 size(단어 벡터의 차원),\n","    * window(컨텍스트 창의 크기),\n","    * max_vocab_size(고려할 최대 어휘 크기),\n","    * min_count(포함할 단어의 최소 빈도)와 같은 특정 하이퍼파라미터로 훈련됩니다.\n","    * sg : 사용할 훈련 알고리즘 - 1은 skip-gram, 0은 CBOW )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6Y26nTvJ1si"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s42Yr9cbJ3uf"},"outputs":[],"source":["from gensim.models import Word2Vec\n","\n","# Word2Vec 모델 생성\n","wv_model = Word2Vec(\n","\n",")"]},{"cell_type":"markdown","metadata":{"id":"PVSSgw6jz-RH"},"source":["* Word2Vec 모델로부터 데이터를 벡터화하기 위한 함수 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGKxMURH0L0R"},"outputs":[],"source":["# Word2Vec 모델로부터 하나의 문장을 벡터화 시키는 함수 생성\n","def get_sent_embedding(model, embedding_size, tokenized_words):\n","    # 임베딩 벡터를 0으로 초기화\n","    feature_vec = np.zeros((embedding_size,), dtype='float32')\n","    # 단어 개수 초기화\n","    n_words = 0\n","    # 모델 단어 집합 생성\n","    index2word_set = set(model.wv.key_to_index.keys())\n","    # 문장의 단어들을 하나씩 반복\n","    for word in tokenized_words:\n","        # 모델 단어 집합에 해당하는 단어일 경우에만\n","        if word in index2word_set:\n","            # 단어 개수 1 증가\n","            n_words += 1\n","            # 임베딩 벡터에 해당 단어의 벡터를 더함\n","            feature_vec = np.add(feature_vec, model.wv.get_vector(word))\n","    # 단어 개수가 0보다 큰 경우 벡터를 단어 개수로 나눠줌 (평균 임베딩 벡터 계산)\n","    if (n_words > 0):\n","        feature_vec = np.divide(feature_vec, n_words)\n","    return feature_vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZS9j5lgKDGQ"},"outputs":[],"source":["# 문장벡터 데이터 셋 만들기\n","def get_dataset(sentences, model, num_features):\n","    dataset = list()\n","\n","    # 각 문장을 벡터화해서 리스트에 저장\n","    for sent in sentences:\n","        dataset.append(get_sent_embedding(model, num_features, sent))\n","\n","    # 리스트를 numpy 배열로 변환하여 반환\n","    sent_embedding_vectors = np.stack(dataset)\n","\n","    return sent_embedding_vectors"]},{"cell_type":"markdown","metadata":{"id":"DEjSR247a9iw"},"source":["* 이제 학습데이터의 Q를 Word2Vec 모델을 사용하여 벡터화 합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIoehvZna82t"},"outputs":[],"source":["# 학습 데이터의 문장들을 Word2Vec 모델을 사용하여 벡터화\n","train_data_vecs = get_dataset(\n","\n",")"]},{"cell_type":"markdown","metadata":{"id":"l9XFPz28RmlE"},"source":["* 훈련된 Word2Vec 모델을 사용하여 문장 목록에 대한 문장 임베딩을 생성하고 이를 2차원 numpy 배열에 저장합니다.\n","* 그런 다음 이러한 임베딩을 다양한 기계 학습 모델의 입력 기능으로 사용할 수 있습니다"]},{"cell_type":"markdown","metadata":{"id":"NOo2RwzWRr0c"},"source":["### (2) 분류 모델링\n","* 데이터 분할\n","    * x, y\n","        * x : 이전 단계에서 저장된 임베딩벡터\n","        * y : intent 값들\n","    * train, val\n","        * train_test_split 활용\n","* 머신러닝 모델링\n","    * lightGBM, RandomForest 등을 활용하여 학습\n","    * 필요하다면 hyper parameter 튜닝을 시도해도 좋습니다.\n","* validation set으로 검증해 봅시다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vvS1F0EoKTv-"},"outputs":[],"source":["# X와 y 데이터 분리\n","\n","\n","# Train-Test split\n"]},{"cell_type":"markdown","metadata":{"id":"qKuYKmQ2dbP1"},"source":["* 모델1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXDMP5gUbQsr"},"outputs":[],"source":["# LightGBM 분류기 생성\n","\n","# 학습\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CPPQ3lejdWwq"},"outputs":[],"source":["# 예측 및 검증\n"]},{"cell_type":"markdown","metadata":{"id":"uq63HIFjdc8c"},"source":["* 모델2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4GQPYXMncvkh"},"outputs":[],"source":["# RandomForest\n","\n","# 학습\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PEXD7iWmDEem"},"outputs":[],"source":["# 예측 및 검증\n"]},{"cell_type":"markdown","metadata":{"id":"l4eV11r6d99i"},"source":["* 모델 저장하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"po3WLHCCqaki"},"outputs":[],"source":["#학습된 LightGBM 모델 lgbm을 파일로 저장"]},{"cell_type":"markdown","metadata":{"id":"NxeL02Q1TecY"},"source":["### (3) 챗봇 구축"]},{"cell_type":"markdown","metadata":{"id":"thqkcWLsTiwc"},"source":["# 챗봇1\n","* **상세요구사항**\n","    * 챗봇 flow : input 질문 -> 분류 모델로 intent 예측 --> intent에 해당하는 답변 출력\n","        * 하나의 intent 에는 여러 답변이 있습니다. 이중 한가지를 랜덤하게 선택합니다."]},{"cell_type":"markdown","metadata":{"id":"GLHnFhw-3dWl"},"source":["#### 1) 데이터 중 하나에 대해서 테스트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t1O90gcPPpvM"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPgsCfCwT06g"},"outputs":[],"source":["# 입력문장 벡터화\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZDfzmPeGT3Om"},"outputs":[],"source":["# 분류 모델을 이용하여 intent 예측\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fof6Zb0UUArm"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ZuF2udm93j7W"},"source":["#### 2) 챗봇 함수 만들기\n","* 테스트 코드를 바탕으로 질문을 받아 답변을 하는 함수를 생성합시다.\n","* 성능이 좋은 모델 사용."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o6YSzGDe3n0M"},"outputs":[],"source":["def get_answer1(question):\n","\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oUi-QgPD4HnP"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"V1a2FMV5yqwi"},"source":["#### 3) test 데이터에 대해서 성능 측정하기\n","\n","test 데이터 전체에 대해서 성능을 측정해 봅시다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B8nXCSHX92HI"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"ufl3SyWzvN53"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 챗봇2\n","\n","* **세부요구사항**\n","    * 단계별 챗봇을 만들어 봅시다.\n","        * 1단계 : type(일상대화 0, 에이블스쿨Q&A 1) 분류 모델 만들기\n","            * Embedding + LSTM 모델\n","        * 2단계 : FastText 모델 생성하여 train의 임베딩벡터 저장\n","        * 3단계 : 챗봇 만들기\n","            * input 문장과 train 임베딩 벡터와 코사인 유사도 계산\n","            * 가장 유사도가 높은 질문의 intent 찾아\n","            * 해당 intent의 답변 중 무작위로 하나를 선정하여 답변하기"],"metadata":{"id":"459-MUm-Qo62"}},{"cell_type":"markdown","metadata":{"id":"XvAzrVuvVQT9"},"source":["### (1) 1단계 : type 분류 모델링(LSTM)\n","- LSTM"]},{"cell_type":"markdown","metadata":{"id":"VklTJM3-tuDQ"},"source":["#### 1) 데이터 준비\n","* 학습용 데이터를 만들어 봅시다.\n","    * 시작 데이터 : clean_train_questions, clean_test_questions\n","    * 각 토큰에 인덱스를 부여하는 토크나이저를 만들고 적용\n","    * 문장별 길이에 대한 분포를 확인하고 적절하게 정의."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TbpMdJT3t228"},"outputs":[],"source":["# 각각의 토큰에 인덱스 부여하는 토크나이저 선언\n","\n","# .fit_on_texts 이용하여 토크나이저 만들기\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ft3qCoGbuIt4"},"outputs":[],"source":["# 전체 토큰의 수 확인\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mOpw0-EwuJ9b"},"outputs":[],"source":["# 전체 토큰의 수가 vocab 사이즈가 됨\n","\n","# fit_on_texts을 위에서 한번만 해도 되지만, vocab 사이즈를 확인하고 줄이거나 하는 시도를 할 수도 있기에 다시 수행\n","\n","# .texts_to_sequences : 토크나이즈 된 데이터를 가지고 모두 시퀀스로 변환\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sVHleK_muYl8"},"outputs":[],"source":["# 각 토큰과 인덱스로 구성된 딕셔너리 생성\n","\n","# <PAD> 는 0으로 추가\n"]},{"cell_type":"markdown","metadata":{"id":"38FwBFRll05a"},"source":["* 문장별 토큰수에 대해 탐색적 분석을 수행해 봅시다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6hwGM_0dubPR"},"outputs":[],"source":["# 문장별 토큰 수 카운트\n","\n","# 기초 통계량\n","\n","# 분포 그래프\n"]},{"cell_type":"markdown","metadata":{"id":"nevMXd0vmj8H"},"source":["* 문장별 토큰이 가장 큰 것이 57개 입니다."]},{"cell_type":"markdown","metadata":{"id":"wQfbaY3HmIMj"},"source":["* 학습 입력을 위한 데이터 크기 맞추기\n","    * 문장이 짧기 때문에 MAX_SEQUENCE_LENGTH는 정하지 않아도 되지만,\n","    * 그러나 분포를 보고 적절하게 자릅시다.\n","    * 그리고 pad_sequences 함수를 이용하여 시퀀스데이터로 변환하기\n","* y는 train['type'] 와 test['type'] 입니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8M2QWrD-ups7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6nlnE9dju5Dh"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMlo9o1VngW2"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"WlBeVPA_6ePq"},"source":["#### 2) 모델링\n","\n","* 토크나이징 한 데이터를 입력으로 받아\n","* Embedding 레이어와 LSTM 레이어를 결합하여 이진 분류 모델링을 수행합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ilRqzlKScHr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0CaxzPmySjte"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11jhI4eOSlOI"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBeQBXhzSm0y"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KOhx2rzGFmiy"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ebdw6-K9AR_V"},"source":["### (2) FastText 모델 : 사전학습된 Word2Vec 모델을 사용"]},{"cell_type":"markdown","metadata":{"id":"9RQnEBcnAZNt"},"source":["-  FastText 모델 학습을 위한 입력 포맷 2차원 리스트 형태 입니다.\n","  ```\n","  [['나', '는', '학생', '이다'], ['오늘', '은', '날씨', '가', '좋다']]\n","  ```"]},{"cell_type":"markdown","metadata":{"id":"ghuzogkoAZNt"},"source":["- Word2Vec계열의 FastText를 학습하는 이유\n","  - n-gram이 추가된 fasttext 모델은 유사한 단어에 대한 임베딩을 word2vec보다 잘 해결할 수 있으며, 오탈자 등에 대한 임베딩 처리가 가능하다.\n","  - 예) 체크카드, 쳌카드는 word2vec에서는 전혀 다른 단어이지만 fasttext는 character n-gram으로 비교적 같은 단어로 처리할 수 있다.\n","- 참고: https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText\n"]},{"cell_type":"markdown","metadata":{"id":"HTGtH4bPAh-8"},"source":["#### 1) 데이터 준비\n","* 시작데이터 : clean_train_questions, clean_test_questions"]},{"cell_type":"markdown","metadata":{"id":"cQIJ4IPZ0b9C"},"source":["* FastText를 위한 입력 데이터 구조 만들기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y-kEQiETAZNt"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"46o8NG_LAk7c"},"source":["#### 2) FastText 모델 생성\n","* FastText 문법\n","    * FastText( input데이터,  min_count = , size= , window=  )\n","        * input데이터 : 학습에 사용할 문장으로 이루어진 리스트\n","        * min_count : 모델에 사용할 단어의 최소 빈도수. 이 값보다 적게 출현한 단어는 모델에 포함되지 않음. 기본값 = 5\n","        * size : 단어의 벡터 차원 지정. 기본값 = 100\n","        * window : 학습할 때 한 단어의 좌우 몇 개의 단어를 보고 예측을 할 것인지를 지정. 기본값 = 5\n","    * 참조 : https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlWLTr2nAZNu"},"outputs":[],"source":["from gensim.models.fasttext import FastText\n","import gensim.models.word2vec\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NilgkdYPAo0s"},"source":["#### 3) train에 임베딩벡터 결과 저장\n","* get_sent_embedding 함수를 이용하여 train의 질문별 임베딩 결과를 저장합니다.\n","    * .apply(lambda .....) 를 활용하세요."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2iZx5hRAZNu"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"HUoQgTf46PJD"},"source":["### (3) 챗봇 구축\n","- input 질문\n","- intent classifier로 common와 faq 중 하나를 예측\n","- 예측된 intent에 속한 train의 모든 Q와 input 문장의 임베딩 벡터간의 코사인 유사도 계산\n","- 코사인 유사도가 가장 높은 top-3개의 Q를 선택\n","- 선택한 Q에 맵핑된 답변 중 하나를 선택하고 실제 답변과 비교"]},{"cell_type":"markdown","metadata":{"id":"esu6rcsJ6PJD"},"source":["#### 1) 하나의 질문으로 테스트해보기"]},{"cell_type":"markdown","metadata":{"id":"dCxYTX2W6PJD"},"source":["* 선택된 질문과 답변"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Nl6O3Di6PJD"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"g-8Ck-Mh6PJD"},"source":["* 예측을 위한 입력 형태로 변환"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0yGO9yQ6PJE"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"-yp2MBmo6PJE"},"source":["* 예측하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BYm1AODK6PJE"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"uF0D3SH16PJE"},"source":["* 질문에 대한 벡터 만들기\n","    * FestText 모델로 부터 벡터 만들기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i_6k-Tjf6PJE"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"rgEVj1WG6PJE"},"source":["* train의 질문 벡터들과 유사도 계산\n","    * FastText 로 만들 벡터들과 유사도 계산"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OSX-j8WV6PJE"},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity\n"]},{"cell_type":"markdown","metadata":{"id":"q-IJcMH26PJF"},"source":["#### 2) 함수로 생성하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYlaEeNN6PJF"},"outputs":[],"source":["def get_answer2(question):\n","\n","\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gONYObLpBwgt"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"GNfo5m5Hyswe"},"source":["#### 3) test 데이터에 대해서 성능 측정하기\n","\n","test 데이터 전체에 대해서 성능을 측정해 봅시다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IKFBp5LMBwgt"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"_qwoDQ6kzBal"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BEjQHVAJCKkN"},"source":["## 5.질문에 대한 답변 비교해보기\n","\n","* **세부요구사항**\n","    * 세가지 챗봇을 생성해 보았습니다.\n","    * 질문을 입력하여 답변을 비교해 봅시다. 어떤 챗봇이 좀 더 정확한 답변을 하나요?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0wqcvSpCOnH"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8q4Vu5CCvSP"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1UOEgZzvQgMas8PiW_0ppe5OLukpNgJAh","timestamp":1696553402622}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}